# å‰¯æœ¬ç®¡ç†ï¼ˆReplica Managementï¼‰

> åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„å‰¯æœ¬æ”¾ç½®ã€æ•…éšœæ¢å¤å’ŒåŠ¨æ€ç®¡ç†æœºåˆ¶

## ç›®å½•

- [å‰¯æœ¬ç®¡ç†ï¼ˆReplica Managementï¼‰](#å‰¯æœ¬ç®¡ç†replica-management)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ](#-æ ¸å¿ƒæ¦‚å¿µ)
    - [å‰¯æœ¬æ”¾ç½®ç­–ç•¥](#å‰¯æœ¬æ”¾ç½®ç­–ç•¥)
    - [æ•…éšœæ¢å¤æœºåˆ¶](#æ•…éšœæ¢å¤æœºåˆ¶)
    - [åŠ¨æ€ç®¡ç†](#åŠ¨æ€ç®¡ç†)
  - [ğŸ”§ å®ç°æœºåˆ¶](#-å®ç°æœºåˆ¶)
    - [å‰¯æœ¬ç®¡ç†å™¨](#å‰¯æœ¬ç®¡ç†å™¨)
    - [æ•…éšœæ£€æµ‹](#æ•…éšœæ£€æµ‹)
    - [æ•°æ®è¿ç§»](#æ•°æ®è¿ç§»)
  - [ğŸš€ é«˜çº§ç‰¹æ€§](#-é«˜çº§ç‰¹æ€§)
    - [æ™ºèƒ½å‰¯æœ¬æ”¾ç½®](#æ™ºèƒ½å‰¯æœ¬æ”¾ç½®)
    - [è‡ªé€‚åº”æ¢å¤](#è‡ªé€‚åº”æ¢å¤)
  - [ğŸ§ª æµ‹è¯•ç­–ç•¥](#-æµ‹è¯•ç­–ç•¥)
    - [å‰¯æœ¬ç®¡ç†æµ‹è¯•](#å‰¯æœ¬ç®¡ç†æµ‹è¯•)
  - [ğŸ” æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
    - [å‰¯æœ¬ä¼˜åŒ–](#å‰¯æœ¬ä¼˜åŒ–)
  - [ğŸ“š è¿›ä¸€æ­¥é˜…è¯»](#-è¿›ä¸€æ­¥é˜…è¯»)
  - [ğŸ”— ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)

## ğŸ“‹ æ¦‚è¿°

å‰¯æœ¬ç®¡ç†æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç¡®ä¿æ•°æ®å¯ç”¨æ€§å’Œä¸€è‡´æ€§çš„æ ¸å¿ƒæœºåˆ¶ã€‚
å®ƒè´Ÿè´£å‰¯æœ¬çš„åˆ›å»ºã€æ”¾ç½®ã€æ•…éšœæ£€æµ‹ã€æ¢å¤å’ŒåŠ¨æ€è°ƒæ•´ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å„ç§æ•…éšœæƒ…å†µä¸‹ä»èƒ½æ­£å¸¸è¿è¡Œã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### å‰¯æœ¬æ”¾ç½®ç­–ç•¥

**å®šä¹‰ 1ï¼ˆå‰¯æœ¬æ”¾ç½®ï¼‰**: å‰¯æœ¬æ”¾ç½®æ˜¯æŒ‡å°†æ•°æ®å‰¯æœ¬åˆ†é…åˆ°ä¸åŒèŠ‚ç‚¹çš„è¿‡ç¨‹ï¼Œéœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š

- **æ•…éšœåŸŸåˆ†ç¦»**: å‰¯æœ¬åº”åˆ†å¸ƒåœ¨ä¸åŒçš„æ•…éšœåŸŸä¸­
- **è´Ÿè½½å‡è¡¡**: å‰¯æœ¬åˆ†å¸ƒåº”ä¿æŒè´Ÿè½½å‡è¡¡
- **ç½‘ç»œæ‹“æ‰‘**: è€ƒè™‘ç½‘ç»œå»¶è¿Ÿå’Œå¸¦å®½é™åˆ¶
- **å­˜å‚¨å®¹é‡**: ç¡®ä¿èŠ‚ç‚¹æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´

### æ•…éšœæ¢å¤æœºåˆ¶

**å®šä¹‰ 2ï¼ˆæ•…éšœæ¢å¤ï¼‰**: æ•…éšœæ¢å¤æ˜¯æŒ‡å½“å‰¯æœ¬èŠ‚ç‚¹å‘ç”Ÿæ•…éšœæ—¶ï¼Œç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºæ–°å‰¯æœ¬å¹¶æ¢å¤æ•°æ®çš„è¿‡ç¨‹ã€‚

### åŠ¨æ€ç®¡ç†

**å®šä¹‰ 3ï¼ˆåŠ¨æ€ç®¡ç†ï¼‰**: åŠ¨æ€ç®¡ç†æ˜¯æŒ‡ç³»ç»Ÿæ ¹æ®è´Ÿè½½ã€å®¹é‡å’Œæ€§èƒ½éœ€æ±‚åŠ¨æ€è°ƒæ•´å‰¯æœ¬æ•°é‡å’Œä½ç½®çš„èƒ½åŠ›ã€‚

## ğŸ”§ å®ç°æœºåˆ¶

### å‰¯æœ¬ç®¡ç†å™¨

```rust
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::time::{SystemTime, UNIX_EPOCH, Duration};

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ReplicaState {
    Healthy,
    Degraded,
    Failed,
    Recovering,
}

#[derive(Debug, Clone)]
pub struct ReplicaInfo {
    pub replica_id: String,
    pub node_id: String,
    pub state: ReplicaState,
    pub last_heartbeat: u64,
    pub data_size: u64,
    pub load_score: f64,
}

#[derive(Debug, Clone)]
pub struct PlacementConstraint {
    pub min_replicas: usize,
    pub max_replicas: usize,
    pub fault_domains: Vec<String>,
    pub preferred_nodes: Vec<String>,
    pub excluded_nodes: Vec<String>,
}

pub struct ReplicaManager {
    replicas: Arc<RwLock<HashMap<String, Vec<ReplicaInfo>>>>,
    nodes: Arc<RwLock<HashMap<String, NodeInfo>>>,
    placement_constraints: Arc<RwLock<HashMap<String, PlacementConstraint>>>,
    heartbeat_timeout: Duration,
    recovery_timeout: Duration,
}

#[derive(Debug, Clone)]
pub struct NodeInfo {
    pub node_id: String,
    pub capacity: u64,
    pub used_capacity: u64,
    pub load_score: f64,
    pub fault_domain: String,
    pub last_seen: u64,
}

impl ReplicaManager {
    pub fn new(heartbeat_timeout: Duration, recovery_timeout: Duration) -> Self {
        Self {
            replicas: Arc::new(RwLock::new(HashMap::new())),
            nodes: Arc::new(RwLock::new(HashMap::new())),
            placement_constraints: Arc::new(RwLock::new(HashMap::new())),
            heartbeat_timeout,
            recovery_timeout,
        }
    }
    
    // æ·»åŠ èŠ‚ç‚¹
    pub fn add_node(&self, node_info: NodeInfo) -> Result<(), Box<dyn std::error::Error>> {
        let mut nodes = self.nodes.write().unwrap();
        nodes.insert(node_info.node_id.clone(), node_info);
        Ok(())
    }
    
    // åˆ›å»ºå‰¯æœ¬
    pub fn create_replicas(&self, data_id: String, constraint: PlacementConstraint) 
        -> Result<Vec<String>, Box<dyn std::error::Error>> {
        let mut replicas = self.replicas.write().unwrap();
        let nodes = self.nodes.read().unwrap();
        
        // é€‰æ‹©å‰¯æœ¬èŠ‚ç‚¹
        let selected_nodes = self.select_replica_nodes(&constraint, &nodes)?;
        
        let mut replica_ids = Vec::new();
        for node_id in selected_nodes {
            let replica_id = format!("{}_{}", data_id, node_id);
            let replica_info = ReplicaInfo {
                replica_id: replica_id.clone(),
                node_id: node_id.clone(),
                state: ReplicaState::Healthy,
                last_heartbeat: SystemTime::now()
                    .duration_since(UNIX_EPOCH)?
                    .as_millis() as u64,
                data_size: 0,
                load_score: 0.0,
            };
            
            replicas.entry(data_id.clone()).or_insert_with(Vec::new).push(replica_info);
            replica_ids.push(replica_id);
        }
        
        // ä¿å­˜æ”¾ç½®çº¦æŸ
        let mut constraints = self.placement_constraints.write().unwrap();
        constraints.insert(data_id, constraint);
        
        Ok(replica_ids)
    }
    
    // é€‰æ‹©å‰¯æœ¬èŠ‚ç‚¹
    fn select_replica_nodes(&self, constraint: &PlacementConstraint, nodes: &HashMap<String, NodeInfo>) 
        -> Result<Vec<String>, Box<dyn std::error::Error>> {
        let mut selected_nodes = Vec::new();
        let mut used_fault_domains = Vec::new();
        
        // ä¼˜å…ˆé€‰æ‹©åå¥½èŠ‚ç‚¹
        for node_id in &constraint.preferred_nodes {
            if let Some(node) = nodes.get(node_id) {
                if self.is_node_available(node) && !constraint.excluded_nodes.contains(node_id) {
                    selected_nodes.push(node_id.clone());
                    used_fault_domains.push(node.fault_domain.clone());
                    
                    if selected_nodes.len() >= constraint.min_replicas {
                        break;
                    }
                }
            }
        }
        
        // è¡¥å……å…¶ä»–èŠ‚ç‚¹
        for (node_id, node) in nodes {
            if selected_nodes.len() >= constraint.max_replicas {
                break;
            }
            
            if !selected_nodes.contains(node_id) && 
               !constraint.excluded_nodes.contains(node_id) &&
               self.is_node_available(node) &&
               !used_fault_domains.contains(&node.fault_domain) {
                selected_nodes.push(node_id.clone());
                used_fault_domains.push(node.fault_domain.clone());
            }
        }
        
        if selected_nodes.len() < constraint.min_replicas {
            return Err("Insufficient available nodes".into());
        }
        
        Ok(selected_nodes)
    }
    
    // æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¯ç”¨
    fn is_node_available(&self, node: &NodeInfo) -> bool {
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_millis() as u64;
        
        current_time - node.last_seen < self.heartbeat_timeout.as_millis() as u64
    }
    
    // å¤„ç†å¿ƒè·³
    pub fn handle_heartbeat(&self, node_id: String, replica_id: String) 
        -> Result<(), Box<dyn std::error::Error>> {
        let mut replicas = self.replicas.write().unwrap();
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)?
            .as_millis() as u64;
        
        for (_, replica_list) in replicas.iter_mut() {
            for replica in replica_list {
                if replica.replica_id == replica_id && replica.node_id == node_id {
                    replica.last_heartbeat = current_time;
                    replica.state = ReplicaState::Healthy;
                    break;
                }
            }
        }
        
        Ok(())
    }
    
    // æ£€æµ‹æ•…éšœ
    pub fn detect_failures(&self) -> Result<Vec<String>, Box<dyn std::error::Error>> {
        let mut failed_replicas = Vec::new();
        let mut replicas = self.replicas.write().unwrap();
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)?
            .as_millis() as u64;
        
        for (data_id, replica_list) in replicas.iter_mut() {
            for replica in replica_list {
                if current_time - replica.last_heartbeat > self.heartbeat_timeout.as_millis() as u64 {
                    replica.state = ReplicaState::Failed;
                    failed_replicas.push(replica.replica_id.clone());
                }
            }
        }
        
        Ok(failed_replicas)
    }
    
    // æ¢å¤æ•…éšœå‰¯æœ¬
    pub fn recover_failed_replicas(&self) -> Result<Vec<String>, Box<dyn std::error::Error>> {
        let mut recovered_replicas = Vec::new();
        let mut replicas = self.replicas.write().unwrap();
        let nodes = self.nodes.read().unwrap();
        let constraints = self.placement_constraints.read().unwrap();
        
        for (data_id, replica_list) in replicas.iter_mut() {
            let failed_replicas: Vec<_> = replica_list.iter()
                .filter(|r| matches!(r.state, ReplicaState::Failed))
                .collect();
            
            if let Some(constraint) = constraints.get(data_id) {
                for failed_replica in failed_replicas {
                    // é€‰æ‹©æ–°çš„å‰¯æœ¬èŠ‚ç‚¹
                    let new_nodes = self.select_replica_nodes(constraint, &nodes)?;
                    
                    for new_node_id in new_nodes {
                        if !replica_list.iter().any(|r| r.node_id == new_node_id) {
                            let new_replica_id = format!("{}_{}", data_id, new_node_id);
                            let new_replica = ReplicaInfo {
                                replica_id: new_replica_id.clone(),
                                node_id: new_node_id,
                                state: ReplicaState::Recovering,
                                last_heartbeat: SystemTime::now()
                                    .duration_since(UNIX_EPOCH)
                                    .unwrap()
                                    .as_millis() as u64,
                                data_size: failed_replica.data_size,
                                load_score: 0.0,
                            };
                            
                            replica_list.push(new_replica);
                            recovered_replicas.push(new_replica_id);
                            break;
                        }
                    }
                }
            }
        }
        
        Ok(recovered_replicas)
    }
}
```

### æ•…éšœæ£€æµ‹

```rust
pub struct FailureDetector {
    replica_manager: Arc<ReplicaManager>,
    detection_interval: Duration,
    failure_threshold: u32,
}

impl FailureDetector {
    pub fn new(replica_manager: Arc<ReplicaManager>, detection_interval: Duration, failure_threshold: u32) -> Self {
        Self {
            replica_manager,
            detection_interval,
            failure_threshold,
        }
    }
    
    // å¯åŠ¨æ•…éšœæ£€æµ‹
    pub async fn start_detection(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut interval = tokio::time::interval(self.detection_interval);
        
        loop {
            interval.tick().await;
            
            // æ£€æµ‹æ•…éšœ
            let failed_replicas = self.replica_manager.detect_failures()?;
            
            if !failed_replicas.is_empty() {
                println!("Detected failed replicas: {:?}", failed_replicas);
                
                // è§¦å‘æ¢å¤
                let recovered_replicas = self.replica_manager.recover_failed_replicas()?;
                println!("Recovered replicas: {:?}", recovered_replicas);
            }
        }
    }
}
```

### æ•°æ®è¿ç§»

```rust
pub struct DataMigrator {
    replica_manager: Arc<ReplicaManager>,
    migration_queue: Arc<RwLock<Vec<MigrationTask>>>,
    max_concurrent_migrations: usize,
}

#[derive(Debug, Clone)]
pub struct MigrationTask {
    pub task_id: String,
    pub source_node: String,
    pub target_node: String,
    pub data_id: String,
    pub data_size: u64,
    pub priority: u32,
    pub created_at: u64,
}

impl DataMigrator {
    pub fn new(replica_manager: Arc<ReplicaManager>, max_concurrent_migrations: usize) -> Self {
        Self {
            replica_manager,
            migration_queue: Arc::new(RwLock::new(Vec::new())),
            max_concurrent_migrations,
        }
    }
    
    // æ·»åŠ è¿ç§»ä»»åŠ¡
    pub fn add_migration_task(&self, task: MigrationTask) -> Result<(), Box<dyn std::error::Error>> {
        let mut queue = self.migration_queue.write().unwrap();
        queue.push(task);
        
        // æŒ‰ä¼˜å…ˆçº§æ’åº
        queue.sort_by(|a, b| b.priority.cmp(&a.priority));
        
        Ok(())
    }
    
    // æ‰§è¡Œè¿ç§»
    pub async fn execute_migration(&self, task: &MigrationTask) -> Result<(), Box<dyn std::error::Error>> {
        println!("Starting migration: {} from {} to {}", 
                task.data_id, task.source_node, task.target_node);
        
        // æ¨¡æ‹Ÿæ•°æ®è¿ç§»è¿‡ç¨‹
        tokio::time::sleep(Duration::from_millis(100)).await;
        
        // æ›´æ–°å‰¯æœ¬ä¿¡æ¯
        let mut replicas = self.replica_manager.replicas.write().unwrap();
        if let Some(replica_list) = replicas.get_mut(&task.data_id) {
            for replica in replica_list {
                if replica.node_id == task.target_node {
                    replica.state = ReplicaState::Healthy;
                    break;
                }
            }
        }
        
        println!("Completed migration: {}", task.task_id);
        Ok(())
    }
    
    // å¤„ç†è¿ç§»é˜Ÿåˆ—
    pub async fn process_migration_queue(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut active_migrations = 0;
        let mut migration_tasks = Vec::new();
        
        loop {
            // è·å–å¾…å¤„ç†çš„è¿ç§»ä»»åŠ¡
            {
                let mut queue = self.migration_queue.write().unwrap();
                while active_migrations < self.max_concurrent_migrations && !queue.is_empty() {
                    if let Some(task) = queue.pop() {
                        migration_tasks.push(task);
                        active_migrations += 1;
                    }
                }
            }
            
            // æ‰§è¡Œè¿ç§»ä»»åŠ¡
            for task in migration_tasks.drain(..) {
                let migrator = self.clone();
                tokio::spawn(async move {
                    if let Err(e) = migrator.execute_migration(&task).await {
                        eprintln!("Migration failed: {}", e);
                    }
                });
            }
            
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    }
}

impl Clone for DataMigrator {
    fn clone(&self) -> Self {
        Self {
            replica_manager: self.replica_manager.clone(),
            migration_queue: self.migration_queue.clone(),
            max_concurrent_migrations: self.max_concurrent_migrations,
        }
    }
}
```

## ğŸš€ é«˜çº§ç‰¹æ€§

### æ™ºèƒ½å‰¯æœ¬æ”¾ç½®

```rust
pub struct IntelligentPlacement {
    replica_manager: Arc<ReplicaManager>,
    load_balancer: Arc<RwLock<LoadBalancer>>,
    capacity_planner: Arc<RwLock<CapacityPlanner>>,
}

pub struct LoadBalancer {
    node_loads: HashMap<String, f64>,
    load_threshold: f64,
}

pub struct CapacityPlanner {
    node_capacities: HashMap<String, u64>,
    capacity_threshold: f64,
}

impl IntelligentPlacement {
    pub fn new(replica_manager: Arc<ReplicaManager>) -> Self {
        Self {
            replica_manager,
            load_balancer: Arc::new(RwLock::new(LoadBalancer {
                node_loads: HashMap::new(),
                load_threshold: 0.8,
            })),
            capacity_planner: Arc::new(RwLock::new(CapacityPlanner {
                node_capacities: HashMap::new(),
                capacity_threshold: 0.9,
            })),
        }
    }
    
    // æ™ºèƒ½å‰¯æœ¬æ”¾ç½®
    pub fn intelligent_placement(&self, data_id: String, data_size: u64) 
        -> Result<Vec<String>, Box<dyn std::error::Error>> {
        let nodes = self.replica_manager.nodes.read().unwrap();
        let load_balancer = self.load_balancer.read().unwrap();
        let capacity_planner = self.capacity_planner.read().unwrap();
        
        // è®¡ç®—èŠ‚ç‚¹è¯„åˆ†
        let mut node_scores = Vec::new();
        for (node_id, node) in nodes.iter() {
            let load_score = 1.0 - load_balancer.node_loads.get(node_id).unwrap_or(&0.0);
            let capacity_score = 1.0 - (node.used_capacity as f64 / node.capacity as f64);
            let total_score = load_score * 0.6 + capacity_score * 0.4;
            
            node_scores.push((node_id.clone(), total_score));
        }
        
        // æŒ‰è¯„åˆ†æ’åº
        node_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        
        // é€‰æ‹©æœ€ä½³èŠ‚ç‚¹
        let selected_nodes: Vec<String> = node_scores.iter()
            .take(3) // é€‰æ‹©å‰3ä¸ªèŠ‚ç‚¹
            .map(|(node_id, _)| node_id.clone())
            .collect();
        
        // åˆ›å»ºå‰¯æœ¬
        let constraint = PlacementConstraint {
            min_replicas: 3,
            max_replicas: 5,
            fault_domains: Vec::new(),
            preferred_nodes: selected_nodes,
            excluded_nodes: Vec::new(),
        };
        
        self.replica_manager.create_replicas(data_id, constraint)
    }
}
```

### è‡ªé€‚åº”æ¢å¤

```rust
pub struct AdaptiveRecovery {
    replica_manager: Arc<ReplicaManager>,
    recovery_strategies: HashMap<ReplicaState, RecoveryStrategy>,
    recovery_history: Arc<RwLock<Vec<RecoveryRecord>>>,
}

#[derive(Debug, Clone)]
pub enum RecoveryStrategy {
    Immediate,
    Delayed(Duration),
    Conditional,
    Manual,
}

#[derive(Debug, Clone)]
pub struct RecoveryRecord {
    pub replica_id: String,
    pub failure_time: u64,
    pub recovery_time: u64,
    pub recovery_duration: u64,
    pub success: bool,
}

impl AdaptiveRecovery {
    pub fn new(replica_manager: Arc<ReplicaManager>) -> Self {
        let mut recovery_strategies = HashMap::new();
        recovery_strategies.insert(ReplicaState::Failed, RecoveryStrategy::Immediate);
        recovery_strategies.insert(ReplicaState::Degraded, RecoveryStrategy::Delayed(Duration::from_secs(30)));
        
        Self {
            replica_manager,
            recovery_strategies,
            recovery_history: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    // è‡ªé€‚åº”æ¢å¤
    pub async fn adaptive_recovery(&self, replica_id: String) -> Result<(), Box<dyn std::error::Error>> {
        let replicas = self.replica_manager.replicas.read().unwrap();
        let mut recovery_history = self.recovery_history.write().unwrap();
        
        // æŸ¥æ‰¾å‰¯æœ¬çŠ¶æ€
        let mut replica_state = None;
        for (_, replica_list) in replicas.iter() {
            for replica in replica_list {
                if replica.replica_id == replica_id {
                    replica_state = Some(replica.state.clone());
                    break;
                }
            }
        }
        
        if let Some(state) = replica_state {
            let strategy = self.recovery_strategies.get(&state)
                .unwrap_or(&RecoveryStrategy::Immediate);
            
            match strategy {
                RecoveryStrategy::Immediate => {
                    self.immediate_recovery(replica_id.clone()).await?;
                }
                RecoveryStrategy::Delayed(delay) => {
                    tokio::time::sleep(*delay).await;
                    self.immediate_recovery(replica_id.clone()).await?;
                }
                RecoveryStrategy::Conditional => {
                    if self.should_recover(&replica_id, &recovery_history) {
                        self.immediate_recovery(replica_id.clone()).await?;
                    }
                }
                RecoveryStrategy::Manual => {
                    // éœ€è¦æ‰‹åŠ¨å¹²é¢„
                    println!("Manual recovery required for replica: {}", replica_id);
                }
            }
            
            // è®°å½•æ¢å¤å†å²
            let recovery_record = RecoveryRecord {
                replica_id: replica_id.clone(),
                failure_time: SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .unwrap()
                    .as_millis() as u64,
                recovery_time: SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .unwrap()
                    .as_millis() as u64,
                recovery_duration: 0,
                success: true,
            };
            
            recovery_history.push(recovery_record);
        }
        
        Ok(())
    }
    
    // ç«‹å³æ¢å¤
    async fn immediate_recovery(&self, replica_id: String) -> Result<(), Box<dyn std::error::Error>> {
        println!("Performing immediate recovery for replica: {}", replica_id);
        
        // æ‰§è¡Œæ¢å¤é€»è¾‘
        let recovered_replicas = self.replica_manager.recover_failed_replicas()?;
        println!("Recovered replicas: {:?}", recovered_replicas);
        
        Ok(())
    }
    
    // åˆ¤æ–­æ˜¯å¦åº”è¯¥æ¢å¤
    fn should_recover(&self, replica_id: &str, recovery_history: &[RecoveryRecord]) -> bool {
        let recent_failures = recovery_history.iter()
            .filter(|record| record.replica_id == replica_id)
            .count();
        
        recent_failures < 3 // å¦‚æœæœ€è¿‘å¤±è´¥æ¬¡æ•°å°‘äº3æ¬¡ï¼Œåˆ™æ¢å¤
    }
}
```

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å‰¯æœ¬ç®¡ç†æµ‹è¯•

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_replica_creation() {
        let replica_manager = ReplicaManager::new(
            Duration::from_secs(30),
            Duration::from_secs(60)
        );
        
        // æ·»åŠ èŠ‚ç‚¹
        let node_info = NodeInfo {
            node_id: "node1".to_string(),
            capacity: 1000,
            used_capacity: 0,
            load_score: 0.0,
            fault_domain: "rack1".to_string(),
            last_seen: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64,
        };
        
        replica_manager.add_node(node_info).unwrap();
        
        // åˆ›å»ºå‰¯æœ¬
        let constraint = PlacementConstraint {
            min_replicas: 1,
            max_replicas: 3,
            fault_domains: Vec::new(),
            preferred_nodes: vec!["node1".to_string()],
            excluded_nodes: Vec::new(),
        };
        
        let replica_ids = replica_manager.create_replicas("data1".to_string(), constraint).unwrap();
        assert_eq!(replica_ids.len(), 1);
    }
    
    #[test]
    fn test_failure_detection() {
        let replica_manager = ReplicaManager::new(
            Duration::from_secs(1),
            Duration::from_secs(60)
        );
        
        // æ·»åŠ èŠ‚ç‚¹å’Œå‰¯æœ¬
        let node_info = NodeInfo {
            node_id: "node1".to_string(),
            capacity: 1000,
            used_capacity: 0,
            load_score: 0.0,
            fault_domain: "rack1".to_string(),
            last_seen: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64,
        };
        
        replica_manager.add_node(node_info).unwrap();
        
        let constraint = PlacementConstraint {
            min_replicas: 1,
            max_replicas: 3,
            fault_domains: Vec::new(),
            preferred_nodes: vec!["node1".to_string()],
            excluded_nodes: Vec::new(),
        };
        
        replica_manager.create_replicas("data1".to_string(), constraint).unwrap();
        
        // ç­‰å¾…è¶…æ—¶
        std::thread::sleep(Duration::from_secs(2));
        
        // æ£€æµ‹æ•…éšœ
        let failed_replicas = replica_manager.detect_failures().unwrap();
        assert!(!failed_replicas.is_empty());
    }
    
    #[tokio::test]
    async fn test_data_migration() {
        let replica_manager = Arc::new(ReplicaManager::new(
            Duration::from_secs(30),
            Duration::from_secs(60)
        ));
        
        let migrator = DataMigrator::new(replica_manager, 5);
        
        let migration_task = MigrationTask {
            task_id: "migration1".to_string(),
            source_node: "node1".to_string(),
            target_node: "node2".to_string(),
            data_id: "data1".to_string(),
            data_size: 1000,
            priority: 1,
            created_at: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64,
        };
        
        migrator.add_migration_task(migration_task).unwrap();
        
        // æ‰§è¡Œè¿ç§»
        let task = MigrationTask {
            task_id: "test_migration".to_string(),
            source_node: "node1".to_string(),
            target_node: "node2".to_string(),
            data_id: "data1".to_string(),
            data_size: 1000,
            priority: 1,
            created_at: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64,
        };
        
        let result = migrator.execute_migration(&task).await;
        assert!(result.is_ok());
    }
}
```

## ğŸ” æ€§èƒ½ä¼˜åŒ–

### å‰¯æœ¬ä¼˜åŒ–

```rust
pub struct ReplicaOptimizer {
    replica_manager: Arc<ReplicaManager>,
    optimization_interval: Duration,
    optimization_threshold: f64,
}

impl ReplicaOptimizer {
    pub fn new(replica_manager: Arc<ReplicaManager>, optimization_interval: Duration, optimization_threshold: f64) -> Self {
        Self {
            replica_manager,
            optimization_interval,
            optimization_threshold,
        }
    }
    
    // å¯åŠ¨ä¼˜åŒ–
    pub async fn start_optimization(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut interval = tokio::time::interval(self.optimization_interval);
        
        loop {
            interval.tick().await;
            
            // æ‰§è¡Œä¼˜åŒ–
            self.optimize_replica_placement().await?;
            self.optimize_load_balancing().await?;
            self.optimize_capacity_usage().await?;
        }
    }
    
    // ä¼˜åŒ–å‰¯æœ¬æ”¾ç½®
    async fn optimize_replica_placement(&self) -> Result<(), Box<dyn std::error::Error>> {
        let replicas = self.replica_manager.replicas.read().unwrap();
        let nodes = self.replica_manager.nodes.read().unwrap();
        
        for (data_id, replica_list) in replicas.iter() {
            // è®¡ç®—å½“å‰æ”¾ç½®çš„è´Ÿè½½å‡è¡¡åº¦
            let load_balance_score = self.calculate_load_balance_score(replica_list, &nodes);
            
            if load_balance_score < self.optimization_threshold {
                println!("Optimizing replica placement for data: {}", data_id);
                // æ‰§è¡Œå‰¯æœ¬é‡æ–°æ”¾ç½®
                self.rebalance_replicas(data_id, replica_list, &nodes).await?;
            }
        }
        
        Ok(())
    }
    
    // è®¡ç®—è´Ÿè½½å‡è¡¡è¯„åˆ†
    fn calculate_load_balance_score(&self, replica_list: &[ReplicaInfo], nodes: &HashMap<String, NodeInfo>) -> f64 {
        let mut node_loads = HashMap::new();
        
        for replica in replica_list {
            if let Some(node) = nodes.get(&replica.node_id) {
                *node_loads.entry(&replica.node_id).or_insert(0.0) += replica.load_score;
            }
        }
        
        if node_loads.is_empty() {
            return 1.0;
        }
        
        let loads: Vec<f64> = node_loads.values().cloned().collect();
        let avg_load = loads.iter().sum::<f64>() / loads.len() as f64;
        let variance = loads.iter()
            .map(|load| (load - avg_load).powi(2))
            .sum::<f64>() / loads.len() as f64;
        
        1.0 / (1.0 + variance.sqrt())
    }
    
    // é‡æ–°å¹³è¡¡å‰¯æœ¬
    async fn rebalance_replicas(&self, data_id: &str, replica_list: &[ReplicaInfo], nodes: &HashMap<String, NodeInfo>) 
        -> Result<(), Box<dyn std::error::Error>> {
        // å®ç°å‰¯æœ¬é‡æ–°å¹³è¡¡é€»è¾‘
        println!("Rebalancing replicas for data: {}", data_id);
        Ok(())
    }
    
    // ä¼˜åŒ–è´Ÿè½½å‡è¡¡
    async fn optimize_load_balancing(&self) -> Result<(), Box<dyn std::error::Error>> {
        // å®ç°è´Ÿè½½å‡è¡¡ä¼˜åŒ–é€»è¾‘
        Ok(())
    }
    
    // ä¼˜åŒ–å®¹é‡ä½¿ç”¨
    async fn optimize_capacity_usage(&self) -> Result<(), Box<dyn std::error::Error>> {
        // å®ç°å®¹é‡ä½¿ç”¨ä¼˜åŒ–é€»è¾‘
        Ok(())
    }
}
```

## ğŸ“š è¿›ä¸€æ­¥é˜…è¯»

- [å¤åˆ¶ç­–ç•¥](./README.md) - å¤åˆ¶ç­–ç•¥æ¦‚è¿°
- [å­˜å‚¨æŠ½è±¡](../storage/README.md) - å­˜å‚¨æŠ½è±¡å’Œå®ç°
- [æ•…éšœå¤„ç†](../failure/README.md) - æ•…éšœæ£€æµ‹å’Œå¤„ç†
- [æ‹“æ‰‘ç®¡ç†](../topology/README.md) - æ‹“æ‰‘ç®¡ç†å’Œè·¯ç”±

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å¤åˆ¶ç­–ç•¥](./README.md)
- [å­˜å‚¨æŠ½è±¡](../storage/README.md)
- [æ•…éšœå¤„ç†](../failure/README.md)
- [æ‹“æ‰‘ç®¡ç†](../topology/README.md)
- [ä¸€è‡´æ€§æ¨¡å‹](../consistency/README.md)
- [å…±è¯†æœºåˆ¶](../consensus/README.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-15  
**ç»´æŠ¤è€…**: Rust åˆ†å¸ƒå¼ç³»ç»Ÿé¡¹ç›®ç»„
